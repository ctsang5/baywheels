{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19a8ef71-69bc-4f86-8d7c-12be944a1ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import LineString\n",
    "import contextily as ctx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "from linearmodels import PanelOLS\n",
    "\n",
    "BAYWHEELS = Path(r\"C:\\Users\\chris\\OneDrive\\Desktop\\baywheels_sf.csv\")\n",
    "RIDERSHIP = Path(r\"C:\\Users\\chris\\Downloads\\RidershipbyRouteTableDownload.csv\")\n",
    "\n",
    "GTFS_ROUTES = Path(r\"C:\\Users\\chris\\OneDrive\\Desktop\\routes.txt\")\n",
    "GTFS_TRIPS = Path(r\"C:\\Users\\chris\\OneDrive\\Desktop\\trips.txt\")\n",
    "GTFS_STOPS = Path(r\"C:\\Users\\chris\\OneDrive\\Desktop\\stops.txt\")\n",
    "\n",
    "GTFS_STOP_TIMES = Path(r\"C:\\Users\\chris\\OneDrive\\Desktop\\stop_times.txt\")\n",
    "GTFS_SHAPES = Path(r\"C:\\Users\\chris\\OneDrive\\Desktop\\shapes.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd665cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "baywheels_sf = pd.read_csv(BAYWHEELS, engine = 'pyarrow')         # REMEMBER: A STATION ID CAN HAVE MULTIPLE STATION NAMES BECAUSE THESE STATIONS ARE VERY CLOSE TOGETHER SO I ASSIGNED THEM ALL THE SAME STATION ID  \n",
    "muni = pd.read_csv(RIDERSHIP, engine = 'pyarrow')\n",
    "\n",
    "routes = pd.read_csv(GTFS_ROUTES, engine = 'pyarrow')\n",
    "trips = pd.read_csv(GTFS_TRIPS, engine = 'pyarrow')\n",
    "stops = pd.read_csv(GTFS_STOPS, engine = 'pyarrow')\n",
    "stop_times = pd.read_csv(GTFS_STOP_TIMES, engine = 'pyarrow')\n",
    "shapes = pd.read_csv(GTFS_SHAPES, engine = 'pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7706863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IF A STATION ID HAS MULTIPLE STATION NAMES IT SHOULDN'T MATTER WHICH STATION NAME'S COORDINATES IS CHOSEN BECAUSE I SET THEM TO BE ALL THE SAME \n",
    "baywheels_sf = baywheels_sf[baywheels_sf['end_station_id'] != 'SF-Y7'].copy() \n",
    "baywheels_sf = baywheels_sf[baywheels_sf['started_at'] > '2019-06-01'].copy()\n",
    "\n",
    "bikeshare_stations = baywheels_sf.sort_values('ended_at').drop_duplicates(subset = ['end_station_id'], keep = 'first').copy()\n",
    "bikeshare_stations = bikeshare_stations[['end_station_id', 'end_lat', 'end_lng', 'ended_at']].rename(columns = {'ended_at': 'first_appeared_at'})\n",
    "bikeshare_stations['first_appeared_at'] = pd.to_datetime(bikeshare_stations['first_appeared_at']).dt.normalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56f7bfe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "muni['Month'] = pd.to_datetime(muni['Month'], format = '%B %Y').dt.normalize()\n",
    "\n",
    "muni.dropna(subset = ['Average Daily Boardings'], inplace = True)\n",
    "muni['Average Daily Boardings'] = muni['Average Daily Boardings'].str.replace(',', '').astype('int64')\n",
    "\n",
    "BUS_SERIVCE_CATEGORIES = ['Frequent Local', 'Grid', 'Rapid Bus', 'Connector']\n",
    "muni = muni[muni['Service Category'].isin(BUS_SERIVCE_CATEGORIES)].copy()\n",
    "muni['Route'] = muni['Route'].str.upper()\n",
    "muni = muni[muni['Service Day of the Week'] == 'Weekday'][['Month', 'Route', 'Average Daily Boardings']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a1ef6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "bus_routes = routes[routes['route_type'] == 3].copy()\n",
    "bus_trips = trips[trips['route_id'].isin(bus_routes['route_id'])].copy()\n",
    "bus_stop_times = stop_times[stop_times['trip_id'].isin(bus_trips['trip_id'])].copy()\n",
    "\n",
    "bus_stop_times = bus_stop_times.merge(stops[['stop_id', 'stop_name', 'stop_lat', 'stop_lon']], on = 'stop_id', how = 'left')\n",
    "bus_trips = bus_trips.merge(bus_routes[['route_id', 'route_short_name', 'route_long_name']], on = 'route_id', how = 'left')\n",
    "\n",
    "bus_route_stops = bus_stop_times.merge(bus_trips[['trip_id', 'route_id', 'route_short_name', 'route_long_name', 'direction_id', 'trip_headsign']])\n",
    "\n",
    "route_stops = bus_route_stops.sort_values(['route_id', 'direction_id', 'stop_sequence']).drop_duplicates(['route_id', 'direction_id', 'stop_id'])\n",
    "\n",
    "keep_route_stop_columns = ['stop_id', 'direction_id', 'stop_sequence', 'stop_name', 'route_short_name', 'route_long_name', 'stop_lat', 'stop_lon']\n",
    "route_stops = route_stops[keep_route_stop_columns]\n",
    "\n",
    "route_stops = route_stops[route_stops['direction_id'] == 1].copy()              # PICK A DIRECTION: I PICKED INBOUND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c56a7070",
   "metadata": {},
   "outputs": [],
   "source": [
    "shapes = shapes.sort_values(['shape_id', 'shape_pt_sequence'])\n",
    "\n",
    "bus_shapes = shapes.groupby('shape_id')[['shape_pt_lon', 'shape_pt_lat']].apply(lambda df: LineString(zip(df['shape_pt_lon'], df['shape_pt_lat']))).reset_index(name = 'geometry')\n",
    "bus_shapes_gdf = gpd.GeoDataFrame(bus_shapes, geometry = 'geometry', crs = 'EPSG:4326')\n",
    "\n",
    "bus_shape_routes = bus_shapes_gdf.merge(bus_trips.drop_duplicates('shape_id'), on = 'shape_id', how = 'left')\n",
    "bus_shape_routes = bus_shape_routes[bus_shape_routes['direction_id'] == 1.0]\n",
    "bus_shape_routes = bus_shape_routes[bus_shape_routes['route_id'].notna()].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c58e8a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.animation as animation\n",
    "\n",
    "bus_df = bus_shape_routes\n",
    "bike_df = bikeshare_stations\n",
    "\n",
    "monthly_dates = pd.date_range(start = bike_df['first_appeared_at'].min(), end = bike_df['first_appeared_at'].max(), freq = 'ME')\n",
    "\n",
    "bus_gdf = gpd.GeoDataFrame(bus_df, geometry = 'geometry', crs = 'EPSG: 4326')\n",
    "\n",
    "bus_gdf_3857 = bus_gdf.to_crs(epsg = 3857)\n",
    "\n",
    "bike_gdf = gpd.GeoDataFrame(bike_df, geometry = gpd.points_from_xy(bike_df.end_lng, bike_df.end_lat), crs = 'EPSG:4326')\n",
    "\n",
    "bike_gdf_3857 = bike_gdf.to_crs(epsg = 3857)\n",
    "\n",
    "bike_gdf_3857['x_3857'] = bike_gdf_3857.geometry.x\n",
    "bike_gdf_3857['y_3857'] = bike_gdf_3857.geometry.y\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (15,15))\n",
    "ax.axis('off')\n",
    "fig.subplots_adjust(left = 0, bottom = 0, right = 1, top = 1)\n",
    "\n",
    "bus_gdf_3857.plot(ax = ax, color = 'blue', alpha = 0.2, linewidth = 1)\n",
    "ctx.add_basemap(ax, source = ctx.providers.CartoDB.Positron)\n",
    "\n",
    "scat = ax.scatter([], [], c = 'red', s = 30, alpha = 0.9, edgecolors = 'white', linewidth = 0.5, zorder = 5)\n",
    "date_text = ax.text(0.02, 0.95, '', transform = ax.transAxes, fontsize = 12, bbox = dict(facecolor = 'white', alpha = 0.9, boxstyle = 'round'))\n",
    "\n",
    "def update(frame_date):\n",
    "    current_data = bike_gdf_3857[bike_gdf_3857['first_appeared_at'] <= frame_date]\n",
    "\n",
    "    if not current_data.empty:\n",
    "        scat.set_offsets(np.c_[current_data['x_3857'], current_data['y_3857']])\n",
    "    \n",
    "    date_text.set_text(frame_date.strftime('%B %Y'))\n",
    "    return scat, date_text\n",
    "\n",
    "ani = animation.FuncAnimation(fig, update, frames = monthly_dates, interval = 100, blit = True)\n",
    "output_file = 'sf_bikeshare_growth.gif'\n",
    "ani.save(output_file, writer = 'pillow', fps = 2, dpi = 150)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "334e59a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "muni_route_number_dictionary = {\n",
    "    '21': '6'\n",
    "}\n",
    "\n",
    "muni_route_long_name_dictionary = {\n",
    "    'HAYES': 'HAYES/PARNASSUS',\n",
    "    'HAIGHT/PARNASSUS': 'HAYES/PARNASSUS'\n",
    "}\n",
    "\n",
    "muni_drop_suspended_routes_list = ['JACKSON', 'TOWNSEND', 'VAN NESS']       # SUSPENDED\n",
    "\n",
    "standardize_route_stops_long_name_dictionary = {\n",
    "    'HAYES-PARNASSUS': 'HAYES/PARNASSUS',       # THE 6 HAYES-PERNASSUS REPLACED 21 HAYES AND 6 HAIGHT/PARNASSUS\n",
    "    'HAIGHT-NORIEGA': 'HAIGHT/NORIEGA',\n",
    "    'FOLSOM-PACIFIC': 'FOLSOM/PACIFIC',\n",
    "    'ASHBURY-18TH ST': 'ASHBURY/18TH',\n",
    "    'UNION-STOCKTON': 'UNION/STOCKTON',\n",
    "    'VAN NESS-MISSION': 'VAN NESS/MISSION',\n",
    "    'QUINTARA-24TH STREET': 'QUINTARA/24TH STREET',\n",
    "\n",
    "}\n",
    "\n",
    "route_stops_drop_routes_list = ['BAYVIEW HUNTERS POINT EXPRESS','CALIFORNIA EXPRESS','MARINA EXPRESS','BART EARLY BIRD','BAYSHORE A EXPRESS',\n",
    "                                'BAYSHORE B EXPRESS', 'SAN BRUNO OWL','3RD-19TH AVE OWL','INGLESIDE BUS','OWL TARAVAL',\n",
    "                                'JUDAH BUS','OWL JUDAH','THIRD BUS']\n",
    "\n",
    "\n",
    "muni[['route_number', 'route_long_name']] = muni['Route'].str.split(' ', n = 1, expand = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "101a3629",
   "metadata": {},
   "outputs": [],
   "source": [
    "route_stops['route_long_name'] = route_stops['route_long_name'].replace(standardize_route_stops_long_name_dictionary)\n",
    "muni['route_long_name'] = muni['route_long_name'].replace(muni_route_long_name_dictionary)\n",
    "\n",
    "muni['route_number'] = muni['route_number'].replace(muni_route_number_dictionary)\n",
    "\n",
    "muni = muni[~muni['route_long_name'].isin(muni_drop_suspended_routes_list)]\n",
    "route_stops = route_stops[~route_stops['route_long_name'].isin(route_stops_drop_routes_list)]\n",
    "\n",
    "muni = muni[['Month', 'Average Daily Boardings', 'route_number', 'route_long_name']]\n",
    "route_stops = route_stops[['stop_id', 'stop_name', 'stop_lat', 'stop_lon', 'route_long_name']] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d154732",
   "metadata": {},
   "outputs": [],
   "source": [
    "## get geo dataframes for route_stops and bikeshare_station -> create a 400 meter buffer size for each bus stop -> \n",
    "# -> find which stations fall inside each buffer ->\n",
    "# -> drop duplicate stations that appear in each route -> join each route-month from muni with the stations near that route -> \n",
    "# -> keep only the stations that already exist by that Month -> count unique stations per route-month -> merge back into muni panel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3caf69c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "stops_gdf = gpd.GeoDataFrame(route_stops, geometry = gpd.points_from_xy(route_stops['stop_lon'], route_stops['stop_lat'], crs = 'EPSG:4326').to_crs(epsg = 3857))\n",
    "stations_gdf = gpd.GeoDataFrame(bikeshare_stations, geometry = gpd.points_from_xy(bikeshare_stations['end_lng'], bikeshare_stations['end_lat']), crs = 'EPSG:4326').to_crs(epsg = 3857)\n",
    "\n",
    "stops_buffered = stops_gdf.copy()\n",
    "stops_buffered['geometry'] = stops_buffered.geometry.buffer(400)\n",
    "\n",
    "route_stop_station = gpd.sjoin(stations_gdf, stops_buffered, predicate = 'within', how = 'inner')\n",
    "\n",
    "route_station_pairs = route_stop_station[['route_long_name', 'end_station_id', 'first_appeared_at']].drop_duplicates() \n",
    "# THE STATIONS WITHIN 400 METERS OF AT LEAST ONE STOP ON EACH ROUTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f0cf1c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "route_month = muni[['route_long_name', 'Month']].drop_duplicates()\n",
    "\n",
    "route_month_station = route_month.merge(route_station_pairs, on = 'route_long_name', how = 'left')\n",
    "\n",
    "mask_active = route_month_station['first_appeared_at'] <= route_month_station['Month']\n",
    "route_month_station = route_month_station[mask_active]\n",
    "\n",
    "route_month_station_counts = route_month_station.groupby(['route_long_name', 'Month'])['end_station_id'].nunique().reset_index(name = 'unique_stations_within_400m')\n",
    "\n",
    "muni_with_station_counts = muni.merge(route_month_station_counts, on = ['route_long_name', 'Month'], how = 'left').fillna({'unique_stations_within_400m': 0})\n",
    "\n",
    "muni_with_station_counts['treated'] = (muni_with_station_counts['unique_stations_within_400m'] > 0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cda11e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is there enough routes that start with 0 stations within 400 meters and later gain stations within 400 meters (switchers)?\n",
    "# The coefficient estimate is found only by routes where treatment changes over time (the switchers). Routes that are always treated or never treated\n",
    "# do not contribute to the coefficient estimate\n",
    "\n",
    "df = muni_with_station_counts.copy()\n",
    "\n",
    "route_treatment_summary = (\n",
    "    df.groupby('route_number')['treated'].agg(['min','max','mean'])\n",
    "    .assign(\n",
    "        group = lambda d: np.select(\n",
    "            [(d['max'] == 0), \n",
    "             (d['min'] == 0) & (d['max'] == 1), \n",
    "             (d['min'] == 1)\n",
    "            ], \n",
    "            ['never_treated', 'switcher', 'always_treated'], \n",
    "            default = 'other'\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "route_treatment_counts = route_treatment_summary['group'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d0b9efdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          PanelOLS Estimation Summary                           \n",
      "================================================================================\n",
      "Dep. Variable:          log_boardings   R-squared:                        0.0051\n",
      "Estimator:                   PanelOLS   R-squared (Between):             -0.0414\n",
      "No. Observations:                3134   R-squared (Within):               0.0490\n",
      "Date:                Tue, Nov 18 2025   R-squared (Overall):             -0.0408\n",
      "Time:                        22:50:44   Log-likelihood                   -575.62\n",
      "Cov. Estimator:             Clustered                                           \n",
      "                                        F-statistic:                      15.417\n",
      "Entities:                          44   P-value                           0.0001\n",
      "Avg Obs:                       71.227   Distribution:                  F(1,3010)\n",
      "Min Obs:                       40.000                                           \n",
      "Max Obs:                       106.00   F-statistic (robust):             2.8068\n",
      "                                        P-value                           0.0940\n",
      "Time periods:                      80   Distribution:                  F(1,3010)\n",
      "Avg Obs:                       39.175                                           \n",
      "Min Obs:                       19.000                                           \n",
      "Max Obs:                       45.000                                           \n",
      "                                                                                \n",
      "                             Parameter Estimates                              \n",
      "==============================================================================\n",
      "            Parameter  Std. Err.     T-stat    P-value    Lower CI    Upper CI\n",
      "------------------------------------------------------------------------------\n",
      "treated       -0.1918     0.1145    -1.6754     0.0940     -0.4162      0.0327\n",
      "==============================================================================\n",
      "\n",
      "F-test for Poolability: 443.07\n",
      "P-value: 0.0000\n",
      "Distribution: F(122,3010)\n",
      "\n",
      "Included effects: Entity, Time\n"
     ]
    }
   ],
   "source": [
    "# Run regression on entire sample \n",
    "\n",
    "panel = df.copy()\n",
    "panel = panel.set_index(['route_number', 'Month'])\n",
    "\n",
    "panel['treated'] = (panel['unique_stations_within_400m'] > 0).astype(int)\n",
    "panel['log_boardings'] = np.log(panel['Average Daily Boardings'])\n",
    "\n",
    "model = PanelOLS(dependent = panel['log_boardings'], exog = panel[['treated']], entity_effects = True, time_effects = True)\n",
    "print(model.fit(cov_type = 'clustered', cluster_entity = True))\n",
    "\n",
    "# There's an x percent decline in (log) average daily boardings for routes that go from 0 stations to at least 1 station within 400 meters, controlling for \n",
    "# entity and time fixed effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4e9d33b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          PanelOLS Estimation Summary                           \n",
      "================================================================================\n",
      "Dep. Variable:          log_boardings   R-squared:                        0.0437\n",
      "Estimator:                   PanelOLS   R-squared (Between):              0.0738\n",
      "No. Observations:                3134   R-squared (Within):              -0.1073\n",
      "Date:                Tue, Nov 18 2025   R-squared (Overall):              0.0759\n",
      "Time:                        22:50:44   Log-likelihood                   -513.59\n",
      "Cov. Estimator:             Clustered                                           \n",
      "                                        F-statistic:                      137.58\n",
      "Entities:                          44   P-value                           0.0000\n",
      "Avg Obs:                       71.227   Distribution:                  F(1,3010)\n",
      "Min Obs:                       40.000                                           \n",
      "Max Obs:                       106.00   F-statistic (robust):             22.764\n",
      "                                        P-value                           0.0000\n",
      "Time periods:                      80   Distribution:                  F(1,3010)\n",
      "Avg Obs:                       39.175                                           \n",
      "Min Obs:                       19.000                                           \n",
      "Max Obs:                       45.000                                           \n",
      "                                                                                \n",
      "                                      Parameter Estimates                                      \n",
      "===============================================================================================\n",
      "                             Parameter  Std. Err.     T-stat    P-value    Lower CI    Upper CI\n",
      "-----------------------------------------------------------------------------------------------\n",
      "unique_stations_within_400m     0.0146     0.0031     4.7712     0.0000      0.0086      0.0206\n",
      "===============================================================================================\n",
      "\n",
      "F-test for Poolability: 312.10\n",
      "P-value: 0.0000\n",
      "Distribution: F(122,3010)\n",
      "\n",
      "Included effects: Entity, Time\n"
     ]
    }
   ],
   "source": [
    "# Try a continuous treatment \n",
    "\n",
    "panel = df.copy()\n",
    "panel = panel.set_index(['route_number', 'Month'])\n",
    "\n",
    "panel['treated'] = (panel['unique_stations_within_400m'] > 0).astype(int)\n",
    "panel['log_boardings'] = np.log(panel['Average Daily Boardings'])\n",
    "\n",
    "model = PanelOLS(dependent = panel['log_boardings'], exog = panel[['unique_stations_within_400m']], entity_effects = True, time_effects = True)\n",
    "print(model.fit(cov_type = 'clustered', cluster_entity = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a82bd19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I CAN'T LEARN MUCH ABOUT THE CAUSAL EFFECT FROM THIS SFMTA DATASET ALONE BECAUSE:\n",
    "# ALMOST ALL THE ROUTES ARE ALWAYS TREATED,\n",
    "# THERE ARE ALMOST NO ROUTES THAT SWITCH BETWEEN TREATED AND UNTREATED,\n",
    "# THERE'S LITTLE TO NO VARIATION IN THE NUMBER OF UNIQUE STATIONS WITHIN 400 METERS (unique_stations_within_400m) OVER TIME\n",
    "# in short, need more SFMTA data between 2010-2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0c1e25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dbd71744",
   "metadata": {},
   "source": [
    "# aRCHIVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7300b5-2908-4f94-9926-ca4bf345f4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _haversine_m(phi1, lam1, phi2, lam2):\n",
    "    R = 6_371_000.0\n",
    "    dphi = phi2 - phi1\n",
    "    dlam = lam2 - lam1\n",
    "    a = np.sin(dphi/2.0)**2 + np.cos(phi1)*np.cos(phi2)*np.sin(dlam/2.0)**2\n",
    "    return 2.0*R*np.arcsin(np.sqrt(a))\n",
    "\n",
    "def build_route_station_exposure(route_stops: pd.DataFrame,\n",
    "                                 stations: pd.DataFrame,\n",
    "                                 *,\n",
    "                                 radius_m: int,\n",
    "                                 return_stop_station: bool = True,\n",
    "                                 validate: bool = True):\n",
    "\n",
    "    req_rs = {'route','stop_id','stop_lat','stop_lon'}\n",
    "    req_st = {'station_id','station_lat','station_lon','open_month'}\n",
    "    missing_rs = req_rs - set(route_stops.columns)\n",
    "    missing_st = req_st - set(stations.columns)\n",
    "    if missing_rs or missing_st:\n",
    "        raise KeyError(f\"Missing columns. route_stops: {sorted(missing_rs)} stations: {sorted(missing_st)}\")\n",
    "\n",
    "    rs = route_stops[list(req_rs)].dropna(subset=['route','stop_id','stop_lat','stop_lon']).copy()\n",
    "    st = stations[list(req_st)].dropna(subset=['station_id','station_lat','station_lon']).copy()\n",
    "\n",
    "    # Coerce types and radians\n",
    "    rs['stop_lat'] = pd.to_numeric(rs['stop_lat'], errors='coerce')\n",
    "    rs['stop_lon'] = pd.to_numeric(rs['stop_lon'], errors='coerce')\n",
    "    st['station_lat'] = pd.to_numeric(st['station_lat'], errors='coerce')\n",
    "    st['station_lon'] = pd.to_numeric(st['station_lon'], errors='coerce')\n",
    "\n",
    "    rs = rs.dropna(subset=['stop_lat','stop_lon'])\n",
    "    st = st.dropna(subset=['station_lat','station_lon'])\n",
    "\n",
    "    rs['phi'] = np.radians(rs['stop_lat'].astype(float))\n",
    "    rs['lam'] = np.radians(rs['stop_lon'].astype(float))\n",
    "    st['phi'] = np.radians(st['station_lat'].astype(float))\n",
    "    st['lam'] = np.radians(st['station_lon'].astype(float))\n",
    "    st['open_month'] = pd.to_datetime(st['open_month'], errors='coerce').values.astype('datetime64[M]')\n",
    "\n",
    "    n_route_stops = rs.groupby('route')['stop_id'].nunique().rename('n_route_stops')\n",
    "\n",
    "    exposures = []\n",
    "    stop_station_links = [] if return_stop_station else None\n",
    "\n",
    "    # Pre-extract station arrays once\n",
    "    st_phi = st['phi'].to_numpy()[None, :]          # shape (1, S)\n",
    "    st_lam = st['lam'].to_numpy()[None, :]          # shape (1, S)\n",
    "\n",
    "    for route, grp in rs.groupby('route', sort=False):\n",
    "        g = grp.reset_index(drop=True)\n",
    "        sphi = g['phi'].to_numpy()[:, None]         # shape (N, 1)\n",
    "        slam = g['lam'].to_numpy()[:, None]\n",
    "\n",
    "        # Distances from each stop on this route to every station -> (N, S)\n",
    "        d = _haversine_m(sphi, slam, st_phi, st_lam)\n",
    "\n",
    "        # Per-station summaries for this route\n",
    "        min_dist = d.min(axis=0)                    # (S,)\n",
    "        within = d <= float(radius_m)               # (N, S)\n",
    "        n_within = within.sum(axis=0)               # (S,)\n",
    "\n",
    "        mask = n_within > 0                         # keep only stations that are near at least one stop\n",
    "        if mask.any():\n",
    "            e = pd.DataFrame({\n",
    "                'route': route,\n",
    "                'station_id': st.loc[mask, 'station_id'].to_numpy(),\n",
    "                'station_lat': st.loc[mask, 'station_lat'].to_numpy(),\n",
    "                'station_lon': st.loc[mask, 'station_lon'].to_numpy(),\n",
    "                'open_month': st.loc[mask, 'open_month'].to_numpy(),\n",
    "                'min_dist_m': min_dist[mask],\n",
    "                'n_stops_within_r': n_within[mask]\n",
    "            })\n",
    "            e['n_route_stops'] = int(n_route_stops.get(route, 0))\n",
    "            e['stop_share_within_r'] = e['n_stops_within_r'] / e['n_route_stops'].where(e['n_route_stops'] != 0, np.nan)\n",
    "            exposures.append(e)\n",
    "\n",
    "            if return_stop_station:\n",
    "                # Build stopâ€“station rows only for stations passing mask\n",
    "                idx_st = np.where(mask)[0]\n",
    "                rows = []\n",
    "                for j in idx_st:\n",
    "                    stop_mask = within[:, j]\n",
    "                    if not stop_mask.any():\n",
    "                        continue\n",
    "                    rows.append(pd.DataFrame({\n",
    "                        'route': route,\n",
    "                        'stop_id': g.loc[stop_mask, 'stop_id'].to_numpy(),\n",
    "                        'station_id': st.iloc[j]['station_id'],\n",
    "                        'dist_m': d[stop_mask, j],\n",
    "                        'open_month': st.iloc[j]['open_month']\n",
    "                    }))\n",
    "                if rows:\n",
    "                    stop_station_links.append(pd.concat(rows, ignore_index=True))\n",
    "\n",
    "    exposure_df = pd.concat(exposures, ignore_index=True) if exposures else pd.DataFrame(\n",
    "        columns=['route','station_id','station_lat','station_lon','open_month',\n",
    "                 'min_dist_m','n_stops_within_r','n_route_stops','stop_share_within_r']\n",
    "    )\n",
    "    stop_station_df = (pd.concat(stop_station_links, ignore_index=True)\n",
    "                       if return_stop_station and stop_station_links else None)\n",
    "\n",
    "    if validate and not exposure_df.empty:\n",
    "        assert (exposure_df['min_dist_m'] >= 0).all()\n",
    "        assert (exposure_df['n_stops_within_r'] >= 1).all()\n",
    "        assert (exposure_df['stop_share_within_r'].between(0, 1, inclusive='both') | exposure_df['stop_share_within_r'].isna()).all()\n",
    "\n",
    "    return exposure_df, stop_station_df\n",
    "\n",
    "\n",
    "def route_month_station_counts(panel_index: pd.DataFrame,\n",
    "                               exposure_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    panel_index: columns ['route','month'] with one row per route-month to fill.\n",
    "    exposure_df: output from build_route_station_exposure().\n",
    "    Returns ['route','month','n_stations_within_r','treated'].\n",
    "    \"\"\"\n",
    "    idx = panel_index[['route','month']].copy()\n",
    "    idx['month'] = pd.to_datetime(idx['month'], errors='coerce').values.astype('datetime64[M]')\n",
    "    exp = exposure_df[['route','station_id','open_month']].dropna().copy()\n",
    "    exp['open_month'] = pd.to_datetime(exp['open_month'], errors='coerce').values.astype('datetime64[M]')\n",
    "\n",
    "    out = []\n",
    "    for route, g in idx.groupby('route', sort=False):\n",
    "        months = g['month'].to_numpy().astype('datetime64[M]').astype('int64')\n",
    "        opens  = exp.loc[exp['route'] == route, 'open_month'].to_numpy().astype('datetime64[M]').astype('int64')\n",
    "        opens.sort()\n",
    "        counts = np.searchsorted(opens, months, side='right')\n",
    "        tmp = g.copy()\n",
    "        tmp['n_stations_within_r'] = counts\n",
    "        tmp['treated'] = (counts > 0).astype('int8')\n",
    "        out.append(tmp)\n",
    "    return pd.concat(out, ignore_index=True)\n",
    "\n",
    "\n",
    "def route_month_stop_coverage(panel_index: pd.DataFrame,\n",
    "                              route_stops: pd.DataFrame,\n",
    "                              stop_station_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns ['route','month','stop_share_covered'] where the share is the fraction\n",
    "    of a route's stops that are within radius of at least one open station by that month.\n",
    "    \"\"\"\n",
    "    idx = panel_index[['route','month']].copy()\n",
    "    idx['month'] = pd.to_datetime(idx['month'], errors='coerce').values.astype('datetime64[M]')\n",
    "\n",
    "    n_route_stops = (route_stops\n",
    "                     .groupby('route')['stop_id']\n",
    "                     .nunique()\n",
    "                     .rename('n_route_stops'))\n",
    "\n",
    "    # First month a stop becomes covered by any station within radius\n",
    "    cov = stop_station_df[['route','stop_id','open_month']].dropna().copy()\n",
    "    cov['open_month'] = pd.to_datetime(cov['open_month'], errors='coerce').values.astype('datetime64[M]')\n",
    "    cov_first = (cov.groupby(['route','stop_id'], as_index=False)['open_month']\n",
    "                    .min()\n",
    "                    .rename(columns={'open_month':'coverage_start'}))\n",
    "\n",
    "    out = []\n",
    "    for route, g in idx.groupby('route', sort=False):\n",
    "        months = g['month'].to_numpy().astype('datetime64[M]').astype('int64')\n",
    "        starts = (cov_first.loc[cov_first['route'] == route, 'coverage_start'].to_numpy()\n",
    "                  .astype('datetime64[M]').astype('int64'))\n",
    "        starts.sort()\n",
    "        covered = np.searchsorted(starts, months, side='right')  # number of stops covered by month\n",
    "        denom = float(n_route_stops.get(route, 0))\n",
    "        tmp = g.copy()\n",
    "        tmp['stop_share_covered'] = 0.0 if denom == 0 else covered / denom\n",
    "        out.append(tmp)\n",
    "    return pd.concat(out, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef4e690-7009-4932-ab69-77b19f4b6aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "bikeshare_stations['open_month'] = pd.to_datetime(bikeshare_stations['first_appeared_at']).values.astype('datetime64[M]')\n",
    "bikeshare_stations.rename(columns = {'end_station_id': 'station_id', 'end_lng': 'station_lon', 'end_lat':'station_lat'}, inplace = True)\n",
    "\n",
    "exposure200, stop_station200 = build_route_station_exposure(route_stops, bikeshare_stations, radius_m = 25, return_stop_station = True)\n",
    "exposure300, stop_station300 = build_route_station_exposure(route_stops, bikeshare_stations, radius_m = 50, return_stop_station = True)\n",
    "exposure400, stop_station400 = build_route_station_exposure(route_stops, bikeshare_stations, radius_m = 75, return_stop_station = True)\n",
    "exposure500, stop_station500 = build_route_station_exposure(route_stops, bikeshare_stations, radius_m = 100, return_stop_station = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1632e2-5084-4d26-8309-84614d678898",
   "metadata": {},
   "outputs": [],
   "source": [
    "treat_counts = route_month_station_counts(ridership, exposure400) # \n",
    "stop_cov     = route_month_stop_coverage(ridership, route_stops, stop_station400) #\n",
    "\n",
    "for df in (ridership, treat_counts, stop_cov):\n",
    "    df['month'] = pd.to_datetime(df['month'], errors='coerce').dt.to_period('M')\n",
    "    df.dropna(subset=['month'], inplace=True)\n",
    "\n",
    "panel = (ridership\n",
    "         .merge(treat_counts, on=['route','month'], how='left')\n",
    "         .merge(stop_cov,     on=['route','month'], how='left')\n",
    "         .assign(stop_share_covered=lambda d: d['stop_share_covered'].fillna(0.0),\n",
    "                 n_stations_within_r=lambda d: d['n_stations_within_r'].fillna(0).astype(int),\n",
    "                 treated=lambda d: d['treated'].fillna(0).astype('int8')))\n",
    "\n",
    "panel = panel[panel['Service Day of the Week'] == 'Weekday']\n",
    "\n",
    "col = (panel['Average Daily Boardings']\n",
    "         .astype(str)\n",
    "         .str.replace(',', '', regex=False)\n",
    "         .str.strip())\n",
    "panel['Average Daily Boardings'] = pd.to_numeric(col, errors='coerce')\n",
    "panel['log_boardings'] = np.log1p(panel['Average Daily Boardings'].astype(float))\n",
    "panel['month'] = panel['month'].dt.to_timestamp(how = 'start')\n",
    "\n",
    "panel = panel[['month', 'route', 'n_stations_within_r', 'stop_share_covered', 'treated','log_boardings']]\n",
    "panel.rename(columns = {'month':'year_month', 'n_stations_within_r':'stations_within_radius', 'stop_share_covered':'share_of_stops_within_radius'}, inplace = True)\n",
    "panel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0a92bd-97e2-4b98-aff6-28f16d6cf120",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2395fd7-717d-465c-9662-d4480807c58d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from linearmodels import PanelOLS\n",
    "\n",
    "panel = panel.set_index(['route', 'year_month']).sort_index()\n",
    "\n",
    "twfe_model = PanelOLS(panel['log_boardings'], panel[['treated']], entity_effects = True, time_effects = True)\n",
    "twfe_model.fit(cov_type = 'clustered', cluster_entity = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92a02bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "panel['treated'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779ee37b-64f8-49b4-87d3-83491bbfaa22",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30fdb9a5-8933-4070-af1a-cc2f114d2e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = panel.copy()\n",
    "df['year_month'] = pd.to_datetime(df['year_month']).dt.to_period('M')\n",
    "df = df.sort_values(['route', 'year_month'])\n",
    "\n",
    "prev = df.groupby('route')['treated'].shift(fill_value=0)\n",
    "df['became_treated'] = df['treated'].eq(1) & prev.eq(0)\n",
    "\n",
    "first_treat = (\n",
    "    df.loc[df['became_treated'], ['route', 'year_month']]\n",
    "      .groupby('route', as_index=False)\n",
    "      .min()\n",
    "      .rename(columns={'year_month': 'first_treat_month'})\n",
    ")\n",
    "\n",
    "first_treat_all = df[['route']].drop_duplicates().merge(first_treat, on='route', how='left')\n",
    "first_treat.groupby('first_treat_month').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f47081-3661-44bc-9fe6-584c053f0dfd",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1ce6d9-b685-473a-abbe-b84d78c9c3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = panel.copy()\n",
    "\n",
    "df['year_month'] = pd.PeriodIndex(pd.to_datetime(df['year_month']).dt.to_period('M'))\n",
    "df = df.sort_values(['route', 'year_month'])\n",
    "df = df.set_index(['route', 'year_month'])\n",
    "\n",
    "y = df['log_boardings']\n",
    "\n",
    "df_reset = df.reset_index()\n",
    "\n",
    "first_event_by_route = df_reset.loc[df_reset['treated'] > 0].groupby('route')['year_month'].min()\n",
    "df_reset['event_month'] = df_reset['route'].map(first_event_by_route)\n",
    "\n",
    "df_reset['ym_num'] = df_reset['year_month'].dt.year * 12 + df_reset['year_month'].dt.month\n",
    "df_reset['em_num'] = np.where(df_reset['event_month'].isna(), np.nan, df_reset['event_month'].dt.year * 12 + df_reset['event_month'].dt.month)\n",
    "\n",
    "df_reset['event_time'] = df_reset['ym_num'] - df_reset['em_num']\n",
    "df = df_reset.set_index(['route', 'year_month']).sort_index()\n",
    "\n",
    "k_min = -3\n",
    "k_max = 5\n",
    "baseline_k = 0.0\n",
    "event_window = list(range(k_min, k_max + 1))\n",
    "\n",
    "dummy_cols = []\n",
    "for k in event_window:\n",
    "    if k == baseline_k:\n",
    "        continue\n",
    "    col = f'event_k_{k:+d}'\n",
    "    df[col] = (df['event_time'] == k).astype('float64')\n",
    "    dummy_cols.append(col)\n",
    "\n",
    "X = df[dummy_cols]\n",
    "y = df['log_boardings']\n",
    "\n",
    "df = df.copy().reset_index()\n",
    "df['year_month'] = df['year_month'].dt.to_timestamp(how = 'start')\n",
    "\n",
    "df = df.sort_values(['route', 'year_month']).set_index(['route', 'year_month'])\n",
    "\n",
    "y = df['log_boardings'].astype('float64')\n",
    "X = df.filter(like = 'event_k_').astype('float64')\n",
    "\n",
    "model = PanelOLS(y, X, entity_effects = True, time_effects = True)\n",
    "res = model.fit(cov_type = 'clustered', cluster_entity = True, cluster_time = True)\n",
    "\n",
    "params = res.params.filter(like='event_k_')\n",
    "ses = res.std_errors.filter(like='event_k_')\n",
    "pvals = res.pvalues.filter(like='event_k_')\n",
    "k_values = pd.Index(params.index).str.replace('event_k_', '', regex=False).astype(int)\n",
    "\n",
    "event_results = pd.DataFrame({\n",
    "    'k': k_values.values,\n",
    "    'coef': params.values,\n",
    "    'se': ses.values,\n",
    "    'pvalue': pvals.values\n",
    "}).sort_values('k').reset_index(drop=True)\n",
    "event_results['ci_low'] = event_results['coef'] - 1.96 * event_results['se']\n",
    "event_results['ci_high'] = event_results['coef'] + 1.96 * event_results['se']\n",
    "event_results['percent_change'] = 100 * (np.exp(event_results['coef']) - 1)\n",
    "\n",
    "import plotnine as p\n",
    "\n",
    "g = (\n",
    "    p.ggplot(event_results, p.aes(x = 'k', y = 'coef', \n",
    "                              ymin = 'ci_low', ymax = 'ci_high')) +\\\n",
    "    p.geom_hline(yintercept = 0, linetype = 'dotted') +\\\n",
    "    p.geom_pointrange(color = 'red', size = 0.7) +\\\n",
    "    p.theme_minimal() +\\\n",
    "    p.geom_hline(yintercept = 0, linetype = 'dashed') +\\\n",
    "    p.geom_vline(xintercept = 0, linetype = 'dashed') +\\\n",
    "    p.coord_cartesian(ylim = (-0.6, 0.6)) +\\\n",
    "    p.xlab('Months before and after first bikeshare installation') +\\\n",
    "    p.ylab('log(Average Daily Boardings)') +\\\n",
    "    p.labs(title='Figure 2: Event Study (300 Meters)') +\\\n",
    "    p.theme(figure_size=(13, 8))\n",
    ")\n",
    "\n",
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543de163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rows actually used in the regression\n",
    "valid_idx = y.index[~y.isna() & ~X.isna().any(axis=1)]\n",
    "\n",
    "rows = []\n",
    "for k in range(k_min, k_max + 1):\n",
    "    if k == baseline_k:  # skip the omitted baseline\n",
    "        continue\n",
    "    col = f'event_k_{k:+d}'\n",
    "    xk = X.loc[valid_idx, col]\n",
    "    rows.append({\n",
    "        'k': k,\n",
    "        'n_routes': int(xk[xk == 1].index.get_level_values('route').nunique()),\n",
    "        'n_obs': int(xk.sum())\n",
    "    })\n",
    "\n",
    "counts = pd.DataFrame(rows).sort_values('k')\n",
    "# optional: attach to your coefficients table\n",
    "event_results = event_results.merge(counts, on='k', how='left')\n",
    "event_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76915969-3299-49ec-bd56-806a94530386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# treated routes only\n",
    "d = df_reset.loc[df_reset['event_month'].notna(), ['route','event_time','log_boardings']]\n",
    "\n",
    "# count observed pre months per route in {-3,-2,-1}\n",
    "pre_counts = (\n",
    "    d.loc[d['event_time'].between(-3, -1) & ~d['log_boardings'].isna()]\n",
    "     .groupby('route').size()\n",
    ")\n",
    "\n",
    "# include treated routes with zero pre months\n",
    "pre_counts = pre_counts.reindex(d['route'].unique(), fill_value=0)\n",
    "\n",
    "# result\n",
    "n_routes_lacking = int((pre_counts < 3).sum())\n",
    "routes_lacking = pre_counts[pre_counts < 3].index.tolist()\n",
    "n_routes_lacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd4cbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "event_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2afc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "EARTH_RADIUS_METERS = 6371000\n",
    "def haversine_distance(bus_latitude: float, bus_longitude: float, station_latitude: float, station_longitude: float, radius: int):\n",
    "\n",
    "    bus_latitude = np.radians(bus_latitude)\n",
    "    bus_longitude = np.radians(bus_longitude)\n",
    "    station_latitude = np.radians(station_latitude)\n",
    "    station_longitude = np.radians(station_longitude)\n",
    "\n",
    "    difference_latitude = station_latitude - bus_latitude\n",
    "    difference_longitude = station_longitude - bus_longitude\n",
    "\n",
    "    half_chord_squared = (np.sin(difference_latitude / 2.0)**2) + (np.cos(bus_latitude)) * (np.cos(station_latitude)) * (np.sin(difference_longitude / 2.0)**2)\n",
    "    angular_distance = 2 * (np.arcsin(np.sqrt(half_chord_squared)))\n",
    "\n",
    "    return radius * angular_distance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
